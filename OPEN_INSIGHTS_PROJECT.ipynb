{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Divyaanshvats/Open-Insights---DS---Gen.AI---Task/blob/main/OPEN_INSIGHTS_PROJECT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1EQHwaH9nMY"
      },
      "source": [
        "# **INSTALLATION OF ALL THE REQUIRED LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wIhcZUf9ib8",
        "outputId": "787dc689-9843-410b-f6c8-dde3769cc15b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (1.0.16)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.7)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.14.1)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.22.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.4)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.74.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (33.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.11.1)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.25.0)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.26.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.36.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.57b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.34.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.7)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.26.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install chromadb\n",
        "!pip install numpy\n",
        "!pip install pymupdf\n",
        "!pip install requests\n",
        "!pip install Pillow\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6H_ac349DXW"
      },
      "source": [
        "# **LOADING THE PDF AND EXTRACTING ITS CONTENT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4R41HKzG7R_o"
      },
      "outputs": [],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import os\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    all_text = []\n",
        "\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc.load_page(page_num)\n",
        "        text = page.get_text()\n",
        "        all_text.append(text)\n",
        "\n",
        "    return all_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmWfYurm-bcd",
        "outputId": "77ab5b63-977b-41d2-8b6d-8822b622ed84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pages: 11\n",
            "\n",
            "--- Page 1 ---\n",
            "Attention Is All You Need\n",
            "Ashish Vaswani∗\n",
            "Google Brain\n",
            "avaswani@google.com\n",
            "Noam Shazeer∗\n",
            "Google Brai\n",
            "\n",
            "\n",
            "--- Page 2 ---\n",
            "Recurrent models typically factor computation along the symbol positions of the input and output\n",
            "seq\n",
            "\n",
            "\n",
            "--- Page 3 ---\n",
            "Figure 1: The Transformer - model architecture.\n",
            "wise fully connected feed-forward network. We employ\n",
            "\n",
            "\n",
            "--- Page 4 ---\n",
            "Scaled Dot-Product Attention\n",
            "Multi-Head Attention\n",
            "Figure 2: (left) Scaled Dot-Product Attention. (ri\n",
            "\n",
            "\n",
            "--- Page 5 ---\n",
            "MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\n",
            "where headi = Attention(QW Q\n",
            "i , KW K\n",
            "i , V W V\n",
            "i\n",
            "\n",
            "\n",
            "--- Page 6 ---\n",
            "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\n",
            "for \n",
            "\n",
            "\n",
            "--- Page 7 ---\n",
            "the input sequence centered around the respective output position. This would increase the maximum\n",
            "p\n",
            "\n",
            "\n",
            "--- Page 8 ---\n",
            "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\n",
            "En\n",
            "\n",
            "\n",
            "--- Page 9 ---\n",
            "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the b\n",
            "\n",
            "\n",
            "--- Page 10 ---\n",
            "References\n",
            "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv pre\n",
            "\n",
            "\n",
            "--- Page 11 ---\n",
            "[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\n",
            "base\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    pdf_path='/content/NIPS-2017-attention-is-all-you-need-Paper.pdf'\n",
        "    pages = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    print(f\"Total pages: {len(pages)}\\n\")\n",
        "\n",
        "    for i, page_text in enumerate(pages):\n",
        "        print(f\"--- Page {i+1} ---\")\n",
        "        print(page_text[:100])  # print first 100 chars\n",
        "        print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **THIS TIME FOR IMAGES**"
      ],
      "metadata": {
        "id": "HzbKTtRN7AAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "def extract_images_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    images = []\n",
        "\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc[page_num]\n",
        "        for img_index, img in enumerate(page.get_images(full=True)):\n",
        "            xref = img[0]\n",
        "            base_image = doc.extract_image(xref)\n",
        "            image_bytes = base_image[\"image\"]\n",
        "            img_pil = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "            images.append({\n",
        "                \"page\": page_num + 1,\n",
        "                \"image\": img_pil,\n",
        "                \"name\": f\"page{page_num+1}_img{img_index+1}\"\n",
        "            })\n",
        "\n",
        "    return images\n"
      ],
      "metadata": {
        "id": "hA_jdxUG7F5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/NIPS-2017-attention-is-all-you-need-Paper.pdf\"\n",
        "images = extract_images_from_pdf(pdf_path)\n",
        "\n",
        "print(f\"Extracted {len(images)} images\")\n",
        "images[0][\"image\"].show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAepeYJK7I04",
        "outputId": "be47cfda-9813-4e2c-b61e-e980e3e649d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 3 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBAFf5rbFu7N"
      },
      "source": [
        "# **LOADING MODEL FROM HUGGING FACE(EMBEDDING MODEL)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frWNuilnHMqY"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"MY_ORIGINAL_TOKEN\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class ClipEmbedder:\n",
        "    def __init__(self, model_name=\"openai/clip-vit-base-patch32\"):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = CLIPModel.from_pretrained(model_name).to(self.device)\n",
        "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
        "\n",
        "    def encode_text(self, texts):\n",
        "        inputs = self.processor(text=texts, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.get_text_features(**inputs)\n",
        "        return outputs.cpu().numpy()\n",
        "\n",
        "    def encode_images(self, images):\n",
        "        inputs = self.processor(images=images, return_tensors=\"pt\").to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.get_image_features(**inputs)\n",
        "        return outputs.cpu().numpy()"
      ],
      "metadata": {
        "id": "2esWiBdGNAjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6597zRjNrCf",
        "outputId": "25edb0bb-a693-4486-d465-59572ad7208f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 512)\n"
          ]
        }
      ],
      "source": [
        "embedder = ClipEmbedder()\n",
        "image_embeddings = embedder.encode_images([img[\"image\"] for img in images])  # your 'images' list\n",
        "print(image_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXz_TDDEPbNx"
      },
      "source": [
        "# **Import ChromaDB and Create and Populate the Vector Store**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23596864",
        "outputId": "97babe83-fbdc-4c63-cea6-becb484082e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flash_attn in /usr/local/lib/python3.11/dist-packages (2.8.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash_attn) (2.6.0+cu124)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash_attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flash_attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash_attn) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install flash_attn#highly optimized attention mechanism designed for use in Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LETS USE OUR EMBEDDING [openai/clip-vit-base-patch32]**"
      ],
      "metadata": {
        "id": "CzIkNz_-FzDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "import os\n",
        "\n",
        "def extract_images_from_pdf(pdf_path, image_dir=\"extracted_images\"):\n",
        "    # Create folder if it doesn't exist\n",
        "    os.makedirs(image_dir, exist_ok=True)\n",
        "\n",
        "    doc = fitz.open(pdf_path)\n",
        "    image_count = 0\n",
        "\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc.load_page(page_num)\n",
        "        images = page.get_images(full=True)\n",
        "\n",
        "        for img_index, img in enumerate(images):\n",
        "            xref = img[0]\n",
        "            base_image = doc.extract_image(xref)\n",
        "            image_bytes = base_image[\"image\"]\n",
        "            image_ext = base_image[\"ext\"]\n",
        "            image_filename = f\"page{page_num+1}_img{img_index+1}.{image_ext}\"\n",
        "\n",
        "            # Save image to folder\n",
        "            with open(os.path.join(image_dir, image_filename), \"wb\") as img_file:\n",
        "                img_file.write(image_bytes)\n",
        "\n",
        "            image_count += 1\n",
        "\n",
        "    print(f\"✅ Extracted {image_count} images from {pdf_path} into '{image_dir}'\")\n",
        "    return image_dir\n"
      ],
      "metadata": {
        "id": "IpuPwK0fazAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    pdf_path = \"/content/NIPS-2017-attention-is-all-you-need-Paper.pdf\"\n",
        "    img_folder = extract_images_from_pdf(pdf_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8VxA1ojazgg",
        "outputId": "d386caf6-a581-4fc2-f0cc-ffe19a757343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Extracted 3 images from /content/NIPS-2017-attention-is-all-you-need-Paper.pdf into 'extracted_images'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import numpy as np\n",
        "\n",
        "# Load CLIP model once (for both text and image embeddings)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "def extract_and_embed_images(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    image_embeddings = []\n",
        "    metadata = []\n",
        "\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc.load_page(page_num)\n",
        "        images = page.get_images(full=True)\n",
        "\n",
        "        for img_index, img in enumerate(images):\n",
        "            xref = img[0]\n",
        "            base_image = doc.extract_image(xref)\n",
        "            image_bytes = base_image[\"image\"]\n",
        "\n",
        "            # Load as PIL image\n",
        "            pil_img = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "\n",
        "            # Create embedding\n",
        "            inputs = clip_processor(images=pil_img, return_tensors=\"pt\").to(device)\n",
        "            with torch.no_grad():\n",
        "                emb = clip_model.get_image_features(**inputs)\n",
        "            emb = emb.cpu().numpy().flatten()\n",
        "\n",
        "            image_embeddings.append(emb)\n",
        "            metadata.append({\n",
        "                \"page\": page_num + 1,\n",
        "                \"type\": \"image\",\n",
        "                \"description\": f\"Image {img_index+1} from page {page_num+1}\"\n",
        "            })\n",
        "\n",
        "    print(f\"✅ Embedded {len(image_embeddings)} images from {pdf_path}\")\n",
        "    return np.array(image_embeddings), metadata\n"
      ],
      "metadata": {
        "id": "EVv5YN8Fb-Dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    pdf_path = \"/content/NIPS-2017-attention-is-all-you-need-Paper.pdf\"\n",
        "    img_embeddings, img_metadata = extract_and_embed_images(pdf_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5h0Gyc8tcRT6",
        "outputId": "75f7cb7f-e560-4b6a-9330-c8b924dc0ba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Embedded 3 images from /content/NIPS-2017-attention-is-all-you-need-Paper.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_text_chunks(text_pages, chunk_size=20):\n",
        "    \"\"\"\n",
        "    text_pages: list of strings (one per page)\n",
        "    chunk_size: approx. number of words per chunk for embedding\n",
        "    \"\"\"\n",
        "    text_embeddings = []\n",
        "    metadata = []\n",
        "\n",
        "    for page_num, page_text in enumerate(text_pages):\n",
        "        words = page_text.split()\n",
        "\n",
        "        # Split into chunks\n",
        "        for i in range(0, len(words), chunk_size):\n",
        "            chunk = \" \".join(words[i:i+chunk_size]).strip()\n",
        "            if not chunk:\n",
        "                continue\n",
        "\n",
        "            # Embed the text chunk\n",
        "            inputs = clip_processor(text=[chunk], return_tensors=\"pt\", padding=True).to(device)\n",
        "            with torch.no_grad():\n",
        "                emb = clip_model.get_text_features(**inputs)\n",
        "            emb = emb.cpu().numpy().flatten()\n",
        "\n",
        "            text_embeddings.append(emb)\n",
        "            metadata.append({\n",
        "                \"page\": page_num + 1,\n",
        "                \"type\": \"text\",\n",
        "                \"content\": chunk\n",
        "            })\n",
        "\n",
        "    print(f\"✅ Embedded {len(text_embeddings)} text chunks from {len(text_pages)} pages\")\n",
        "    return np.array(text_embeddings), metadata"
      ],
      "metadata": {
        "id": "GPjUaZk3cVHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    pdf_path = \"/content/NIPS-2017-attention-is-all-you-need-Paper.pdf\"\n",
        "\n",
        "    # 1. Extract text\n",
        "    pages = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    # 2. Embed text chunks\n",
        "    text_embeddings, text_metadata = embed_text_chunks(pages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-R_n_6eOcdpC",
        "outputId": "ba867dd3-8596-49d5-c7be-02f1d4b999e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Embedded 255 text chunks from 11 pages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "\n",
        "def store_embeddings_in_chroma(text_embeddings, text_metadata, img_embeddings, img_metadata, collection_name=\"attention_rag\"):\n",
        "    client = chromadb.Client()\n",
        "\n",
        "    # Create or get collection\n",
        "    collection = client.get_or_create_collection(name=collection_name)\n",
        "\n",
        "    # Add text embeddings\n",
        "    for idx, (emb, meta) in enumerate(zip(text_embeddings, text_metadata)):\n",
        "        collection.add(\n",
        "            ids=[f\"text_{idx}\"],\n",
        "            embeddings=[emb.tolist()],\n",
        "            metadatas=[meta],\n",
        "            documents=[meta[\"content\"]]\n",
        "        )\n",
        "\n",
        "    # Add image embeddings\n",
        "    for idx, (emb, meta) in enumerate(zip(img_embeddings, img_metadata)):\n",
        "        collection.add(\n",
        "            ids=[f\"image_{idx}\"],\n",
        "            embeddings=[emb.tolist()],\n",
        "            metadatas=[meta],\n",
        "            documents=[meta[\"description\"]]\n",
        "        )\n",
        "\n",
        "    print(f\"✅ Stored {len(text_embeddings)} text chunks and {len(img_embeddings)} images in Chroma collection '{collection_name}'\")\n",
        "    return collection"
      ],
      "metadata": {
        "id": "Fy8ZTuQAcfRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection = store_embeddings_in_chroma(text_embeddings, text_metadata, img_embeddings, img_metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCVQuZOCc9CR",
        "outputId": "8fb8c3c5-344d-43bb-970c-986817f71de6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Stored 255 text chunks and 3 images in Chroma collection 'attention_rag'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(query, collection, top_k=5):\n",
        "    # Embed query\n",
        "    inputs = clip_processor(text=[query], return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        query_emb = clip_model.get_text_features(**inputs).cpu().numpy().flatten()\n",
        "\n",
        "    results = collection.query(\n",
        "        query_embeddings=[query_emb.tolist()],\n",
        "        n_results=top_k\n",
        "    )\n",
        "    return results"
      ],
      "metadata": {
        "id": "wbXWfUB1c-tR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NOW LETS RUN THE 1ST QUERY FROM OUR DATABASE**"
      ],
      "metadata": {
        "id": "itDM7gvl5F0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How does multi-head attention work?\"\n",
        "results = retrieve(query, collection, top_k=5)\n",
        "\n",
        "for i in range(len(results[\"documents\"][0])):\n",
        "    print(f\"\\nResult {i+1}:\")\n",
        "    print(\"Document:\", results[\"documents\"][0][i])\n",
        "    print(\"Metadata:\", results[\"metadatas\"][0][i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztaJSclHdKTR",
        "outputId": "890e5218-06d9-4450-dd9b-fc7b98328615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Result 1:\n",
            "Document: the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described\n",
            "Metadata: {'type': 'text', 'page': 2, 'content': 'the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described'}\n",
            "\n",
            "Result 2:\n",
            "Document: heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of\n",
            "Metadata: {'content': 'heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of', 'page': 7, 'type': 'text'}\n",
            "\n",
            "Result 3:\n",
            "Document: to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
            "Metadata: {'content': 'to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.', 'page': 4, 'type': 'text'}\n",
            "\n",
            "Result 4:\n",
            "Document: MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O where headi = Attention(QW Q i , KW K i ,\n",
            "Metadata: {'content': 'MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O where headi = Attention(QW Q i , KW K i ,', 'page': 5, 'type': 'text'}\n",
            "\n",
            "Result 5:\n",
            "Document: multi-headed self-attention. For translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based on recurrent or convolutional layers.\n",
            "Metadata: {'type': 'text', 'page': 9, 'content': 'multi-headed self-attention. For translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based on recurrent or convolutional layers.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import io\n",
        "import base64\n",
        "\n",
        "def retrieve_with_visuals(query, collection, top_k=5):\n",
        "    # Embed query\n",
        "    inputs = clip_processor(text=[query], return_tensors=\"pt\", padding=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        query_emb = clip_model.get_text_features(**inputs).cpu().numpy().flatten()\n",
        "\n",
        "    # Search in Chroma\n",
        "    results = collection.query(\n",
        "        query_embeddings=[query_emb.tolist()],\n",
        "        n_results=top_k\n",
        "    )\n",
        "\n",
        "    # Display results\n",
        "    for i in range(len(results[\"documents\"][0])):\n",
        "        print(f\"\\nResult {i+1}:\")\n",
        "        print(\"Metadata:\", results[\"metadatas\"][0][i])\n",
        "\n",
        "        if results[\"metadatas\"][0][i].get(\"type\") == \"image\":\n",
        "            # Decode base64 back to image\n",
        "            img_data = base64.b64decode(results[\"documents\"][0][i])\n",
        "            img = Image.open(io.BytesIO(img_data))\n",
        "            display(img)\n",
        "        else:\n",
        "            print(\"Text:\", results[\"documents\"][0][i])\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "g5LCbeuedOMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **THIS WILL BE OUR 2ND QUERY**"
      ],
      "metadata": {
        "id": "e0wmNMvt5MWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"scaled dot-product attention diagram\"\n",
        "results = retrieve_with_visuals(query, collection, top_k=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YfuVkthddB-",
        "outputId": "7e475e60-015a-4d42-e63a-f997c4da9296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Result 1:\n",
            "Metadata: {'content': 'query with the corresponding key. 3.2.1 Scaled Dot-Product Attention We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The', 'page': 3, 'type': 'text'}\n",
            "Text: query with the corresponding key. 3.2.1 Scaled Dot-Product Attention We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The\n",
            "\n",
            "Result 2:\n",
            "Metadata: {'content': 'attention [2], and dot-product (multi- plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of', 'type': 'text', 'page': 4}\n",
            "Text: attention [2], and dot-product (multi- plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of\n",
            "\n",
            "Result 3:\n",
            "Metadata: {'page': 5, 'content': 'property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input', 'type': 'text'}\n",
            "Text: property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\n",
            "\n",
            "Result 4:\n",
            "Metadata: {'content': 'Scaled Dot-Product Attention Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running', 'page': 4, 'type': 'text'}\n",
            "Text: Scaled Dot-Product Attention Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running\n",
            "\n",
            "Result 5:\n",
            "Metadata: {'type': 'text', 'content': 'dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected, bigger models are better,', 'page': 9}\n",
            "Text: dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected, bigger models are better,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    print(type(model))\n",
        "    print(\"Model device:\", next(model.parameters()).device)\n",
        "except NameError:\n",
        "    print(\"Model not loaded yet.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhynCRPgrr3x",
        "outputId": "1f4bd596-96a8-441d-cd4e-eb5619d9aa12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGeneration'>\n",
            "Model device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for doc, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
        "    print(\"\\n---- Retrieved Segment ----\")\n",
        "    print(doc)\n",
        "    print(\"Metadata:\", meta)\n",
        "\n",
        "    if meta.get(\"type\") == \"image\":\n",
        "        # Process image segment\n",
        "        image = None\n",
        "        if \"image_path\" in meta:\n",
        "            try:\n",
        "                image = Image.open(meta[\"image_path\"]).convert(\"RGB\")\n",
        "                blip2_output = generate_with_blip2(\n",
        "                    prompt=f\"Explain this diagram in relation to: {query}\",\n",
        "                    image=image\n",
        "                )\n",
        "                print(\"\\nBLIP2 Output (Image):\", blip2_output)\n",
        "            except FileNotFoundError:\n",
        "                 print(f\"Warning: Image file not found at {meta['image_path']}. Skipping image processing.\")\n",
        "                 print(\"\\nCould not process image.\")\n",
        "        else:\n",
        "             print(\"\\nImage metadata missing 'image_path'. Cannot process image.\")\n",
        "\n",
        "    else:\n",
        "        # Process text segment - simply print the context\n",
        "        print(\"\\nRetrieved Text Context:\", doc)\n",
        "        # If you have a text-based LLM, you could use it here\n",
        "        # blip2_output = text_llm_generate(f\"Using the following context, answer the query:\\n\\nContext: {doc}\\n\\nQuery: {query}\")\n",
        "        # print(\"\\nText LLM Output:\", blip2_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKPsLz1H5S8C",
        "outputId": "be57d157-f953-4410-b27e-a5688e680943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---- Retrieved Segment ----\n",
            "query with the corresponding key. 3.2.1 Scaled Dot-Product Attention We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The\n",
            "Metadata: {'type': 'text', 'content': 'query with the corresponding key. 3.2.1 Scaled Dot-Product Attention We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The', 'page': 3}\n",
            "\n",
            "Retrieved Text Context: query with the corresponding key. 3.2.1 Scaled Dot-Product Attention We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The\n",
            "\n",
            "---- Retrieved Segment ----\n",
            "attention [2], and dot-product (multi- plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of\n",
            "Metadata: {'content': 'attention [2], and dot-product (multi- plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of', 'page': 4, 'type': 'text'}\n",
            "\n",
            "Retrieved Text Context: attention [2], and dot-product (multi- plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of\n",
            "\n",
            "---- Retrieved Segment ----\n",
            "property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\n",
            "Metadata: {'type': 'text', 'content': 'property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input', 'page': 5}\n",
            "\n",
            "Retrieved Text Context: property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOAD THE **Salesforce/blip2-opt-6.7b Model**, this is totally for **Vision part**. That is for the given input image generate the text as output.\n",
        "\n",
        "Function: Load BLIP-2 with quantization fallbacks and generate descriptions from an image with an optional text prompt."
      ],
      "metadata": {
        "id": "YVJyVA8m73TP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# robust BLIP-2 loader + image+text generation (copy-paste)\n",
        "import torch\n",
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration, BitsAndBytesConfig\n",
        "from PIL import Image\n",
        "import traceback\n",
        "\n",
        "model_id = \"Salesforce/blip2-opt-6.7b\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "def load_blip2_with_fallback(model_id):\n",
        "    # Try 4-bit, then 8-bit, then float16/32\n",
        "    try:\n",
        "        print(\"Trying 4-bit (bitsandbytes)...\")\n",
        "        bnb_4bit = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16\n",
        "        )\n",
        "        model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "            model_id,\n",
        "            quantization_config=bnb_4bit,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        print(\"Loaded in 4-bit.\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(\"4-bit failed:\", e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "    try:\n",
        "        print(\"Trying 8-bit (bitsandbytes)...\")\n",
        "        bnb_8bit = BitsAndBytesConfig(load_in_8bit=True)\n",
        "        model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "            model_id,\n",
        "            quantization_config=bnb_8bit,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        print(\"Loaded in 8-bit.\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(\"8-bit failed:\", e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "    try:\n",
        "        print(\"Trying float16/float32 device_map='auto' ...\")\n",
        "        dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
        "        model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=dtype,\n",
        "            device_map=\"auto\" if device==\"cuda\" else None,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        print(\"Loaded in float16/float32.\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(\"float load failed:\", e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "    raise RuntimeError(\"All loading strategies failed.\")\n",
        "\n",
        "# Load processor + model\n",
        "print(\"Loading processor (Blip2Processor) ...\")\n",
        "processor = Blip2Processor.from_pretrained(model_id, trust_remote_code=True)\n",
        "print(\"Loading model ... (this can take a while)\")\n",
        "model = load_blip2_with_fallback(model_id)\n",
        "model.eval()\n",
        "\n",
        "model_device = next(model.parameters()).device\n",
        "print(\"Model device:\", model_device)\n",
        "\n",
        "# Helper to generate + debug\n",
        "def describe_image(image_path, prompt=\"\", do_sample=False, max_new_tokens=200):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    # Prepare inputs (NOT moving to device yet so we can inspect tokens)\n",
        "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\")\n",
        "    # Debug: token ids for the prompt (if present)\n",
        "    if \"input_ids\" in inputs:\n",
        "        tok = inputs[\"input_ids\"][0]\n",
        "        print(\"input_ids (first 50):\", tok[:50].tolist())\n",
        "        # quick pad-check:\n",
        "        pad_id = processor.tokenizer.pad_token_id\n",
        "        if pad_id is not None and torch.all(tok == pad_id):\n",
        "            print(\"WARNING: all prompt tokens are PAD (token id = {}).\".format(pad_id))\n",
        "\n",
        "    # Move to model device and correct dtype\n",
        "    device = next(model.parameters()).device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Generate (with optional sampling if deterministic output is echoing)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=do_sample,\n",
        "            top_p=0.9 if do_sample else None,\n",
        "            temperature=0.7 if do_sample else None,\n",
        "            use_cache=True\n",
        "        )\n",
        "    decoded = processor.tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    return decoded\n",
        "\n",
        "# Example run - update path if needed\n",
        "image_path = \"/content/TRANSFORMER.png\"   # your local image\n",
        "prompt = \"Explain this diagram in detail, focusing on how multi-head attention works.\"\n",
        "\n",
        "print(\"\\n=== Running image+prompt generation ===\")\n",
        "result = describe_image(image_path, prompt=prompt, do_sample=True, max_new_tokens=250)\n",
        "print(\"\\n=== Generated ===\\n\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780,
          "referenced_widgets": [
            "47d94c13d7db4bc7907e2e1b4684c81e",
            "51d94e808de14b82ab7654ced6dc58ce",
            "b365327e5a9f4ae08aa35e40145833a3",
            "48248263d97e4c62a25d2af0e33542f8",
            "2340219a9c7e4b4fa3d73d712ba039b6",
            "0f2e26c8d52a4d11b3b1f7bcd9df1ca2",
            "796a3a6c64044b0fb420569478c5379b",
            "552a196be55a4ee18bd19530b22553f0",
            "ae5573c44b4741d2aa2a6414324ed4dd",
            "26b7d300b9354f40aec7a0c23f03fdf8",
            "225de9926d21410ba736976700ae6284"
          ]
        },
        "id": "U6zRIp5XULLk",
        "outputId": "3e806502-014e-4360-e340-517022e82099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Loading processor (Blip2Processor) ...\n",
            "Loading model ... (this can take a while)\n",
            "Trying 4-bit (bitsandbytes)...\n",
            "4-bit failed: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\n",
            "Trying 8-bit (bitsandbytes)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2371346253.py\", line 21, in load_blip2_with_fallback\n",
            "    model = Blip2ForConditionalGeneration.from_pretrained(\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 316, in _wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 4879, in from_pretrained\n",
            "    hf_quantizer.validate_environment(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\", line 76, in validate_environment\n",
            "    raise ImportError(\n",
            "ImportError: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8-bit failed: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\n",
            "Trying float16/float32 device_map='auto' ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2371346253.py\", line 36, in load_blip2_with_fallback\n",
            "    model = Blip2ForConditionalGeneration.from_pretrained(\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 316, in _wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 4879, in from_pretrained\n",
            "    hf_quantizer.validate_environment(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_8bit.py\", line 73, in validate_environment\n",
            "    raise ImportError(\n",
            "ImportError: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47d94c13d7db4bc7907e2e1b4684c81e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded in float16/float32.\n",
            "Model device: cpu\n",
            "\n",
            "=== Running image+prompt generation ===\n",
            "input_ids (first 50): [50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 2, 43043, 1851, 42, 41071, 11, 4617, 6, 5650, 15, 141, 3228, 12, 3628, 1503, 1364, 4]\n",
            "\n",
            "=== Generated ===\n",
            " Explain this diagram in detail, focusing on how multi-head attention works.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GENERATION OF TEXT, BASED ON OUR INPUT IMAGE**\n",
        "Produce a generic caption from the image alone, or answer a question about the image using a “Question: … Answer\""
      ],
      "metadata": {
        "id": "hoGjheJy5haF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Image-only captioning\n",
        "result = describe_image(\n",
        "    \"/content/TRANSFORMER.png\",\n",
        "    prompt=\"\",  # empty for pure caption\n",
        "    do_sample=True,\n",
        "    max_new_tokens=200\n",
        ")\n",
        "print(\"\\n=== Image-only Caption ===\\n\", result)\n",
        "\n",
        "# Image+prompt Q&A style\n",
        "prompt = \"Question: Explain this diagram in detail, focusing on how multi-head attention works. Answer:\"\n",
        "result_qna = describe_image(\n",
        "    \"/content/TRANSFORMER.png\",\n",
        "    prompt=prompt,\n",
        "    do_sample=True,\n",
        "    max_new_tokens=250\n",
        ")\n",
        "print(\"\\n=== Q&A Caption ===\\n\", result_qna)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfOaZCBMULnQ",
        "outputId": "1eed4e9d-8636-4ba6-c9ef-dac745fabb95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids (first 50): [50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 2]\n",
            "\n",
            "=== Image-only Caption ===\n",
            " the block diagram for a data processing system\n",
            "\n",
            "input_ids (first 50): [50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 2, 45641, 35, 44109, 42, 41071, 11, 4617, 6, 5650, 15, 141, 3228, 12, 3628, 1503, 1364, 4]\n",
            "\n",
            "=== Q&A Caption ===\n",
            " Question: Explain this diagram in detail, focusing on how multi-head attention works. Answer: Multi-head attention is the ability of a computer to recognize the presence of more than one object at the same time\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GENERATION OF TEXT BASED ON OUR INPUT(WHICH IS TEXT)**  \n",
        "For this we laod another model and that is GPT-2, which is lightweight and works pretty well."
      ],
      "metadata": {
        "id": "HYfueJO65na6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c309ccc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "044f732e-2121-424f-93bf-823386bb9454"
      },
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# ------------------------\n",
        "# Load BLIP2 (already loaded earlier)\n",
        "# ------------------------\n",
        "# model, processor = ... # You already have this part\n",
        "\n",
        "# ------------------------\n",
        "# Load lightweight text model for RAG text generation\n",
        "# ------------------------\n",
        "text_model_name = \"gpt2\"  # small & fast, replace with bigger if GPU available\n",
        "text_tokenizer = AutoTokenizer.from_pretrained(text_model_name)\n",
        "text_model = AutoModelForCausalLM.from_pretrained(text_model_name).to(\"cpu\")\n",
        "\n",
        "# ------------------------\n",
        "# Function to generate from text-only context\n",
        "# ------------------------\n",
        "def generate_text_only(context, query, max_tokens=80):\n",
        "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    inputs = text_tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\")\n",
        "    output_ids = text_model.generate(**inputs, max_new_tokens=max_tokens)\n",
        "    return text_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# ------------------------\n",
        "# Main RAG pipeline\n",
        "# ------------------------\n",
        "def rag_pipeline(query, retrieved_docs):\n",
        "    \"\"\"\n",
        "    retrieved_docs: list of dicts\n",
        "      Each dict has:\n",
        "        - type: 'image' or 'text'\n",
        "        - content: path (if image) or text string\n",
        "    \"\"\"\n",
        "    answers = []\n",
        "    for doc in retrieved_docs:\n",
        "        if doc['type'] == 'image':\n",
        "            image = Image.open(doc['content'])\n",
        "            inputs = processor(images=image, text=query, return_tensors=\"pt\").to(\"cpu\")\n",
        "            output_ids = model.generate(**inputs, max_new_tokens=50)\n",
        "            description = processor.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "            answers.append({\"type\": \"image\", \"output\": description})\n",
        "\n",
        "        elif doc['type'] == 'text':\n",
        "            answer = generate_text_only(doc['content'], query)\n",
        "            answers.append({\"type\": \"text\", \"output\": answer})\n",
        "\n",
        "    return answers\n",
        "\n",
        "# ------------------------\n",
        "# Example Usage\n",
        "# ------------------------\n",
        "retrieved_docs = [\n",
        "    {\"type\": \"text\", \"content\": \"Scaled dot-product attention is computed as QK^T / sqrt(d_k) ...\"}\n",
        "]\n",
        "\n",
        "query = \"Explain the transformer architecture\"\n",
        "results = rag_pipeline(query, retrieved_docs)\n",
        "\n",
        "for res in results:\n",
        "    print(f\"[{res['type'].upper()} RESULT] {res['output']}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEXT RESULT] Context: Scaled dot-product attention is computed as QK^T / sqrt(d_k) ...\n",
            "\n",
            "Question: Explain the transformer architecture\n",
            "Answer: The transformer architecture is a set of functions that are used to compute the magnitude of the product of the two products. The functions are:\n",
            "\n",
            "QK^T = QK^T / sqrt(d_k)\n",
            "\n",
            "where QK^T is the product of the two products.\n",
            "\n",
            "The function QK^T is a function that takes a product and returns a product\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "47d94c13d7db4bc7907e2e1b4684c81e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_51d94e808de14b82ab7654ced6dc58ce",
              "IPY_MODEL_b365327e5a9f4ae08aa35e40145833a3",
              "IPY_MODEL_48248263d97e4c62a25d2af0e33542f8"
            ],
            "layout": "IPY_MODEL_2340219a9c7e4b4fa3d73d712ba039b6"
          }
        },
        "51d94e808de14b82ab7654ced6dc58ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f2e26c8d52a4d11b3b1f7bcd9df1ca2",
            "placeholder": "​",
            "style": "IPY_MODEL_796a3a6c64044b0fb420569478c5379b",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "b365327e5a9f4ae08aa35e40145833a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_552a196be55a4ee18bd19530b22553f0",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae5573c44b4741d2aa2a6414324ed4dd",
            "value": 4
          }
        },
        "48248263d97e4c62a25d2af0e33542f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26b7d300b9354f40aec7a0c23f03fdf8",
            "placeholder": "​",
            "style": "IPY_MODEL_225de9926d21410ba736976700ae6284",
            "value": " 4/4 [00:02&lt;00:00,  1.86it/s]"
          }
        },
        "2340219a9c7e4b4fa3d73d712ba039b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f2e26c8d52a4d11b3b1f7bcd9df1ca2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "796a3a6c64044b0fb420569478c5379b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "552a196be55a4ee18bd19530b22553f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae5573c44b4741d2aa2a6414324ed4dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "26b7d300b9354f40aec7a0c23f03fdf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "225de9926d21410ba736976700ae6284": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}